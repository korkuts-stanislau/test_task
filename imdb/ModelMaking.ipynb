{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>There is process of making model after analysis and experiments</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Import modules</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Let's load the data</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def get_datasets_from_file(filename, label_column_name, test_size):\n",
    "    if not (0 <= test_size <= 1):\n",
    "        raise Exception(\"train_test_split must be from 0 to 1\")\n",
    "    data = pd.read_csv(filename)\n",
    "    if label_column_name not in data.columns:\n",
    "        raise Exception(f\"There is no column '{label_column_name}' in the data\")\n",
    "    X = data.drop([label_column_name], axis=1)\n",
    "    y = data[label_column_name]\n",
    "    return train_test_split(X, y, test_size=test_size, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = get_datasets_from_file(\"IMDB_dataset.csv\", \"sentiment\", 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>38094</th>\n",
       "      <td>As much as I love trains, I couldn't stomach t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40624</th>\n",
       "      <td>This was a very good PPV, but like Wrestlemani...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49425</th>\n",
       "      <td>Not finding the right words is everybody's pro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35734</th>\n",
       "      <td>I'm really suprised this movie didn't get a hi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41708</th>\n",
       "      <td>I'll start by confessing that I tend to really...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review\n",
       "38094  As much as I love trains, I couldn't stomach t...\n",
       "40624  This was a very good PPV, but like Wrestlemani...\n",
       "49425  Not finding the right words is everybody's pro...\n",
       "35734  I'm really suprised this movie didn't get a hi...\n",
       "41708  I'll start by confessing that I tend to really..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38094    negative\n",
       "40624    positive\n",
       "49425    negative\n",
       "35734    positive\n",
       "41708    negative\n",
       "Name: sentiment, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38094    0\n",
       "40624    1\n",
       "49425    0\n",
       "35734    1\n",
       "41708    0\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train, y_test = y_train.map({\"positive\": 1, \"negative\": 0}), y_test.map({\"positive\": 1, \"negative\": 0})\n",
    "y_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Copy some functions from experiments</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import TransformerMixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from spacy.lang.en import English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuations = string.punctuation\n",
    "stop_words = STOP_WORDS\n",
    "parser = English()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "#function from EDA\n",
    "def spacy_text_normalizer(text):\n",
    "    tokens = re.sub(r\"<.*>\", \"\", text) #Remove all tags\n",
    "    tokens = parser(text) #Get doc from text\n",
    "    tokens = [ word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in tokens ] #Normalize words\n",
    "    tokens = [ word for word in tokens if word not in stop_words and word not in punctuations ] #Remove stop words and punctuation\n",
    "    return \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextNormalizer(TransformerMixin):\n",
    "    def __init__(self, text_column_name=\"review\"):\n",
    "        self.text_column_name = text_column_name\n",
    "        \n",
    "    def transform(self, X, **transform_params):\n",
    "        return [spacy_text_normalizer(text) for text in X[self.text_column_name]]\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "\n",
    "    def get_params(self, deep=True):\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizer = TextNormalizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>I will use logistic regression and tfidf vectorization</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor = TfidfVectorizer()\n",
    "classifier = LogisticRegression()\n",
    "\n",
    "model = Pipeline([\n",
    "            (\"normalizer\", normalizer),\n",
    "            (\"tfidf\", feature_extractor),\n",
    "            (\"logreg\", classifier)\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'tfidf__max_features':[100, 2000],\n",
    "    'tfidf__ngram_range': [(1, 1), (1, 2), (2, 2)],\n",
    "    \n",
    "    'logreg__C': np.logspace(-3,3,3),\n",
    "    'logreg__penalty': [\"l1\",\"l2\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "search = GridSearchCV(model, params, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(estimator=Pipeline(steps=[('normalizer',\n",
       "                                        <__main__.TextNormalizer object at 0x00000223B56FCD88>),\n",
       "                                       ('tfidf', TfidfVectorizer()),\n",
       "                                       ('logreg', LogisticRegression())]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'logreg__C': array([1.e-03, 1.e+00, 1.e+03]),\n",
       "                         'logreg__penalty': ['l1', 'l2'],\n",
       "                         'tfidf__max_features': [100, 2000],\n",
       "                         'tfidf__ngram_range': [(1, 1), (1, 2), (2, 2)]})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'logreg__C': 1.0,\n",
       " 'logreg__penalty': 'l2',\n",
       " 'tfidf__max_features': 2000,\n",
       " 'tfidf__ngram_range': (1, 1)}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8737428571428572"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "metrics = {\n",
    "    \"accuracy\": metrics.accuracy_score,\n",
    "    \"precision\": metrics.precision_score,\n",
    "    \"recall\": metrics.recall_score,\n",
    "    \"f1\": metrics.f1_score\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics\n",
      "accuracy value is 0.8766\n",
      "precision value is 0.8707676402171104\n",
      "recall value is 0.8878640137040453\n",
      "f1 value is 0.8792327265609708\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Metrics\")\n",
    "y_pred = model.predict(X_test)\n",
    "for metric_name, metric in metrics.items():\n",
    "    print(f\"{metric_name} value is {metric(y_test, y_pred)}\")\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Save model</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(model, open(\"model.pickle\", 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Summary</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Results after tuning hyperparameters are lower than in experiments with this combination of extractor and classifier. Maybe we need to continue looking for optimal hyperparameters. For example look for logreg__C in range from -10 to 10 with step 10 or make more tfidf max_features.</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>There would be a video card, would train an LSTM + 1D convolution neural network on keras. I looked on the Internet for a solution on this dataset, where it gives 94% accuracy</h4>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
